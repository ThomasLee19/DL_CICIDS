{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CICIDS2017特征工程与分层数据准备\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# CICIDS2017数据集特征工程笔记本\n",
    "# 用于网络异常检测的深度学习模型\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from collections import Counter\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.utils import resample\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置更好的可视化样式\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# 定义路径\n",
    "INPUT_PATH = '/root/autodl-tmp/projects/DL/dataset/preprocessed/CICIDS2017_merged_preprocessed.csv'\n",
    "OUTPUT_DIR = '/root/autodl-tmp/projects/DL/dataset/feature_engineering'\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"CICIDS2017特征工程与分层数据准备\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. 加载预处理数据\n",
      "--------------------------------------------------\n",
      "正在加载数据: /root/autodl-tmp/projects/DL/dataset/preprocessed/CICIDS2017_merged_preprocessed.csv\n",
      "数据加载完成，耗时: 19.11秒\n",
      "数据集形状: (2830743, 81)\n",
      "内存使用: 907.07 MB\n"
     ]
    }
   ],
   "source": [
    "# 1. 数据加载与初步探索\n",
    "# --------------------------------------------------\n",
    "print(\"\\n1. 加载预处理数据\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"正在加载数据: {INPUT_PATH}\")\n",
    "\n",
    "# 由于数据集较大，使用pandas的chunksize参数分批读取\n",
    "# 但首先读取小样本来确定数据类型\n",
    "sample_df = pd.read_csv(INPUT_PATH, nrows=10000)\n",
    "dtypes = sample_df.dtypes\n",
    "numeric_columns = sample_df.select_dtypes(include=['float64']).columns\n",
    "int_columns = sample_df.select_dtypes(include=['int64']).columns\n",
    "\n",
    "# 设置优化的数据类型\n",
    "optimized_dtypes = {}\n",
    "for col in numeric_columns:\n",
    "    optimized_dtypes[col] = 'float32'  # 降低精度以节省内存\n",
    "for col in int_columns:\n",
    "    optimized_dtypes[col] = 'int32'  # 降低精度以节省内存\n",
    "\n",
    "# 分批读取并合并\n",
    "chunk_size = 500000  # 每批读取的行数\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(INPUT_PATH, chunksize=chunk_size, dtype=optimized_dtypes):\n",
    "    chunks.append(chunk)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"数据加载完成，耗时: {load_time:.2f}秒\")\n",
    "print(f\"数据集形状: {df.shape}\")\n",
    "\n",
    "# 显示内存使用情况\n",
    "memory_usage = df.memory_usage().sum() / (1024 ** 2)\n",
    "print(f\"内存使用: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. 数据探索性分析\n",
      "--------------------------------------------------\n",
      "\n",
      "标签分布:\n",
      "BENIGN                        2273097\n",
      "DOS HULK                       231073\n",
      "PORTSCAN                       158930\n",
      "DDOS                           128027\n",
      "DOS GOLDENEYE                   10293\n",
      "FTP-PATATOR                      7938\n",
      "SSH-PATATOR                      5897\n",
      "DOS SLOWLORIS                    5796\n",
      "DOS SLOWHTTPTEST                 5499\n",
      "BOT                              1966\n",
      "WEB ATTACK � BRUTE FORCE         1507\n",
      "WEB ATTACK � XSS                  652\n",
      "INFILTRATION                       36\n",
      "WEB ATTACK � SQL INJECTION         21\n",
      "HEARTBLEED                         11\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "各攻击类型百分比:\n",
      "BENIGN: 80.3004%\n",
      "DOS HULK: 8.1630%\n",
      "PORTSCAN: 5.6144%\n",
      "DDOS: 4.5227%\n",
      "DOS GOLDENEYE: 0.3636%\n",
      "FTP-PATATOR: 0.2804%\n",
      "SSH-PATATOR: 0.2083%\n",
      "DOS SLOWLORIS: 0.2048%\n",
      "DOS SLOWHTTPTEST: 0.1943%\n",
      "BOT: 0.0695%\n",
      "WEB ATTACK � BRUTE FORCE: 0.0532%\n",
      "WEB ATTACK � XSS: 0.0230%\n",
      "INFILTRATION: 0.0013%\n",
      "WEB ATTACK � SQL INJECTION: 0.0007%\n",
      "HEARTBLEED: 0.0004%\n",
      "\n",
      "数据类型:\n",
      "int32      54\n",
      "float32    24\n",
      "object      3\n",
      "dtype: int64\n",
      "\n",
      "检查缺失值:\n",
      "没有缺失值\n",
      "\n",
      "数值特征的统计摘要:\n",
      "                                     mean           std   min          max  \\\n",
      "Destination Port             8.071483e+03  1.828363e+04   0.0      65535.0   \n",
      "Flow Duration                1.478566e+07  3.365374e+07 -13.0  119999998.0   \n",
      "Total Fwd Packets            9.361160e+00  7.496728e+02   1.0     219759.0   \n",
      "Total Backward Packets       1.039377e+01  9.973883e+02   0.0     291922.0   \n",
      "Total Length of Fwd Packets  5.493024e+02  9.993589e+03   0.0   12900000.0   \n",
      "\n",
      "                                   range  coefficient_of_variation  \n",
      "Destination Port                 65535.0                  2.265214  \n",
      "Flow Duration                120000011.0                  2.276106  \n",
      "Total Fwd Packets               219758.0                 80.083323  \n",
      "Total Backward Packets          291922.0                 95.960201  \n",
      "Total Length of Fwd Packets   12900000.0                 18.193237  \n",
      "完整统计摘要已保存至: /root/autodl-tmp/projects/DL/dataset/feature_engineering/feature_statistics.csv\n"
     ]
    }
   ],
   "source": [
    "# 2. 数据探索性分析\n",
    "# --------------------------------------------------\n",
    "print(\"\\n2. 数据探索性分析\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# 检查标签分布\n",
    "print(\"\\n标签分布:\")\n",
    "label_counts = df['Label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# 可视化标签分布\n",
    "plt.figure(figsize=(14, 8))\n",
    "label_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Attack Types Distribution')\n",
    "plt.xlabel('Attack Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'attack_distribution.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 获取每种攻击类型的百分比\n",
    "attack_percentages = (label_counts / len(df)) * 100\n",
    "print(\"\\n各攻击类型百分比:\")\n",
    "for attack_type, percentage in attack_percentages.items():\n",
    "    print(f\"{attack_type}: {percentage:.4f}%\")\n",
    "\n",
    "# 检查数据类型和缺失值\n",
    "print(\"\\n数据类型:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n检查缺失值:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "if len(missing_values) > 0:\n",
    "    print(missing_values)\n",
    "else:\n",
    "    print(\"没有缺失值\")\n",
    "\n",
    "# 统计分析\n",
    "print(\"\\n数值特征的统计摘要:\")\n",
    "numeric_df = df.select_dtypes(include=['float32', 'float64', 'int32', 'int64'])\n",
    "summary_stats = numeric_df.describe().T\n",
    "summary_stats['range'] = summary_stats['max'] - summary_stats['min']\n",
    "summary_stats['coefficient_of_variation'] = summary_stats['std'] / summary_stats['mean']\n",
    "print(summary_stats[['mean', 'std', 'min', 'max', 'range', 'coefficient_of_variation']].head())\n",
    "\n",
    "# 保存完整统计摘要\n",
    "summary_stats.to_csv(os.path.join(OUTPUT_DIR, 'feature_statistics.csv'))\n",
    "print(f\"完整统计摘要已保存至: {os.path.join(OUTPUT_DIR, 'feature_statistics.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. 创建二分类和多分类标签\n",
      "--------------------------------------------------\n",
      "二分类标签创建完成，分布如下:\n",
      "0    2273097\n",
      "1     557646\n",
      "Name: binary_label, dtype: int64\n",
      "\n",
      "以下罕见攻击类型将被归为'Other Attacks'组（阈值：1000）:\n",
      "- WEB ATTACK � XSS: 652\n",
      "- INFILTRATION: 36\n",
      "- WEB ATTACK � SQL INJECTION: 21\n",
      "- HEARTBLEED: 11\n",
      "\n",
      "多分类标签创建完成，分布如下:\n",
      "Normal           2273097\n",
      "DOS               380688\n",
      "PORTSCAN          158930\n",
      "FTP-PATATOR         7938\n",
      "SSH-PATATOR         5897\n",
      "BOT                 1966\n",
      "Web Attack          1507\n",
      "Other Attacks        720\n",
      "Name: multiclass_label, dtype: int64\n",
      "\n",
      "标签映射:\n",
      "Normal: 0\n",
      "DOS: 1\n",
      "Other Attacks: 2\n",
      "FTP-PATATOR: 3\n",
      "SSH-PATATOR: 4\n",
      "Web Attack: 5\n",
      "BOT: 6\n",
      "PORTSCAN: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/root/autodl-tmp/projects/DL/dataset/feature_engineering/label_mapping.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 创建二分类和多分类标签\n",
    "# --------------------------------------------------\n",
    "print(\"\\n3. 创建二分类和多分类标签\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# 创建二分类标签 (Normal vs Attack)\n",
    "df['binary_label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
    "print(\"二分类标签创建完成，分布如下:\")\n",
    "print(df['binary_label'].value_counts())\n",
    "\n",
    "# 创建多分类标签 - 将罕见攻击类型分组\n",
    "# 定义分组策略 - 将样本量少于1000的攻击类型归为\"Other Attacks\"\n",
    "THRESHOLD = 1000\n",
    "attack_counts = label_counts[label_counts.index != 'BENIGN']\n",
    "rare_attacks = attack_counts[attack_counts < THRESHOLD].index.tolist()\n",
    "\n",
    "print(f\"\\n以下罕见攻击类型将被归为'Other Attacks'组（阈值：{THRESHOLD}）:\")\n",
    "for attack in rare_attacks:\n",
    "    print(f\"- {attack}: {label_counts[attack]}\")\n",
    "\n",
    "# 创建多分类标签\n",
    "def create_multiclass_label(label):\n",
    "    if label == 'BENIGN':\n",
    "        return 'Normal'\n",
    "    elif label in rare_attacks:\n",
    "        return 'Other Attacks'\n",
    "    else:\n",
    "        # 将DOS攻击类型归为一组\n",
    "        if 'DOS' in label:\n",
    "            return 'DOS'\n",
    "        # 将Web攻击类型归为一组\n",
    "        elif 'WEB ATTACK' in label:\n",
    "            return 'Web Attack'\n",
    "        else:\n",
    "            return label\n",
    "\n",
    "df['multiclass_label'] = df['Label'].apply(create_multiclass_label)\n",
    "print(\"\\n多分类标签创建完成，分布如下:\")\n",
    "print(df['multiclass_label'].value_counts())\n",
    "\n",
    "# 将多分类标签编码为数字\n",
    "label_mapping = {label: idx for idx, label in enumerate(df['multiclass_label'].unique())}\n",
    "df['multiclass_encoded'] = df['multiclass_label'].map(label_mapping)\n",
    "\n",
    "print(\"\\n标签映射:\")\n",
    "for label, code in label_mapping.items():\n",
    "    print(f\"{label}: {code}\")\n",
    "\n",
    "# 保存标签映射以便后续使用\n",
    "joblib.dump(label_mapping, os.path.join(OUTPUT_DIR, 'label_mapping.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. 特征工程与选择\n",
      "--------------------------------------------------\n",
      "特征矩阵形状: (2830743, 78)\n",
      "\n",
      "计算特征相关性矩阵...\n",
      "为相关性分析选择了30个高方差特征\n",
      "相关性计算完成，耗时: 15.04秒\n",
      "\n",
      "发现25对高度相关的特征对 (|correlation| > 0.9):\n",
      "- Flow Duration 和 Fwd IAT Total: 0.9986\n",
      "- Fwd IAT Max 和 Flow IAT Max: 0.9981\n",
      "- Fwd IAT Max 和 Idle Max: 0.9884\n",
      "- Fwd IAT Max 和 Idle Mean: 0.9781\n",
      "- Fwd IAT Max 和 Idle Min: 0.9491\n",
      "...以及20对其他高度相关的特征对\n",
      "\n",
      "发现8个常数特征，10个近常数特征\n",
      "常数特征:\n",
      "- Bwd PSH Flags\n",
      "- Bwd URG Flags\n",
      "- Fwd Avg Bytes/Bulk\n",
      "- Fwd Avg Packets/Bulk\n",
      "- Fwd Avg Bulk Rate\n",
      "- Bwd Avg Bytes/Bulk\n",
      "- Bwd Avg Packets/Bulk\n",
      "- Bwd Avg Bulk Rate\n",
      "已移除常数特征，特征矩阵新形状: (2830743, 70)\n",
      "\n",
      "发现0个高基数特征 (唯一值比例>80%):\n"
     ]
    }
   ],
   "source": [
    "# 4. 特征工程与选择\n",
    "# --------------------------------------------------\n",
    "print(\"\\n4. 特征工程与选择\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# 移除标签列和可能导致数据泄露的列\n",
    "features_to_drop = ['Label', 'binary_label', 'multiclass_label', 'multiclass_encoded', 'Day', 'Scenario']\n",
    "X = df.drop(columns=features_to_drop, errors='ignore')\n",
    "y_binary = df['binary_label']\n",
    "y_multi = df['multiclass_encoded']\n",
    "\n",
    "print(f\"特征矩阵形状: {X.shape}\")\n",
    "\n",
    "# 检查特征的相关性\n",
    "print(\"\\n计算特征相关性矩阵...\")\n",
    "correlation_time = time.time()\n",
    "\n",
    "# 为了节省内存，只计算部分特征的相关性\n",
    "# 4.1 选择数值列\n",
    "numeric_cols = X.select_dtypes(include=['float32', 'float64', 'int32', 'int64']).columns\n",
    "\n",
    "# 4.2 如果特征太多，选择前N个\n",
    "MAX_CORRELATION_FEATURES = 30  # 限制相关性分析的特征数量\n",
    "if len(numeric_cols) > MAX_CORRELATION_FEATURES:\n",
    "    # 选择方差最大的特征\n",
    "    variances = X[numeric_cols].var().sort_values(ascending=False)\n",
    "    selected_cols = variances.index[:MAX_CORRELATION_FEATURES].tolist()\n",
    "    print(f\"为相关性分析选择了{MAX_CORRELATION_FEATURES}个高方差特征\")\n",
    "else:\n",
    "    selected_cols = numeric_cols.tolist()\n",
    "\n",
    "# 4.3 计算相关性\n",
    "corr_matrix = X[selected_cols].corr()\n",
    "correlation_time = time.time() - correlation_time\n",
    "print(f\"相关性计算完成，耗时: {correlation_time:.2f}秒\")\n",
    "\n",
    "# 保存相关性矩阵热图\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'correlation_heatmap.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 找出高度相关的特征对\n",
    "corr_pairs = []\n",
    "for i in range(len(selected_cols)):\n",
    "    for j in range(i+1, len(selected_cols)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:  # 高度相关阈值设为0.9\n",
    "            corr_pairs.append((selected_cols[i], selected_cols[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "print(f\"\\n发现{len(corr_pairs)}对高度相关的特征对 (|correlation| > 0.9):\")\n",
    "for feat1, feat2, corr in corr_pairs[:5]:  # 只显示前5对\n",
    "    print(f\"- {feat1} 和 {feat2}: {corr:.4f}\")\n",
    "\n",
    "if len(corr_pairs) > 5:\n",
    "    print(f\"...以及{len(corr_pairs)-5}对其他高度相关的特征对\")\n",
    "\n",
    "# 检测常数和近常数特征\n",
    "const_features = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "near_const_features = [col for col in X.columns if X[col].nunique() <= 2 and X[col].nunique() > 1]\n",
    "\n",
    "print(f\"\\n发现{len(const_features)}个常数特征，{len(near_const_features)}个近常数特征\")\n",
    "if const_features:\n",
    "    print(\"常数特征:\")\n",
    "    for feat in const_features:\n",
    "        print(f\"- {feat}\")\n",
    "    \n",
    "    # 移除常数特征\n",
    "    X = X.drop(columns=const_features)\n",
    "    print(f\"已移除常数特征，特征矩阵新形状: {X.shape}\")\n",
    "\n",
    "# 识别高基数特征（唯一值比例>80%的特征）\n",
    "high_cardinality_cols = []\n",
    "for col in X.columns:\n",
    "    unique_ratio = X[col].nunique() / len(X)\n",
    "    if unique_ratio > 0.8:\n",
    "        high_cardinality_cols.append((col, unique_ratio))\n",
    "\n",
    "print(f\"\\n发现{len(high_cardinality_cols)}个高基数特征 (唯一值比例>80%):\")\n",
    "for col, ratio in high_cardinality_cols[:5]:  # 只显示前5个\n",
    "    print(f\"- {col}: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. 分层数据集划分\n",
      "--------------------------------------------------\n",
      "执行分层数据集划分...\n",
      "数据集划分完成，耗时: 4.19秒\n",
      "\n",
      "训练集: 1981520 样本\n",
      "验证集: 424611 样本\n",
      "测试集: 424612 样本\n",
      "\n",
      "二分类标签分布:\n",
      "训练集: Counter({0: 1591168, 1: 390352})\n",
      "验证集: Counter({0: 340964, 1: 83647})\n",
      "测试集: Counter({0: 340965, 1: 83647})\n",
      "\n",
      "多分类标签分布:\n",
      "训练集: Counter({0: 1591168, 1: 266481, 7: 111251, 3: 5557, 4: 4128, 6: 1376, 5: 1055, 2: 504})\n",
      "验证集: Counter({0: 340964, 1: 57103, 7: 23839, 3: 1191, 4: 885, 6: 295, 5: 226, 2: 108})\n",
      "测试集: Counter({0: 340965, 1: 57104, 7: 23840, 3: 1190, 4: 884, 6: 295, 5: 226, 2: 108})\n"
     ]
    }
   ],
   "source": [
    "# 5. 分层数据集划分\n",
    "# --------------------------------------------------\n",
    "print(\"\\n5. 分层数据集划分\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# 进行分层划分：训练集、验证集和测试集 (70-15-15)\n",
    "print(\"执行分层数据集划分...\")\n",
    "split_time = time.time()\n",
    "\n",
    "# 先划分为训练集和临时集\n",
    "X_train, X_temp, y_binary_train, y_binary_temp, y_multi_train, y_multi_temp = train_test_split(\n",
    "    X, y_binary, y_multi, test_size=0.3, stratify=y_multi, random_state=42\n",
    ")\n",
    "\n",
    "# 再将临时集划分为验证集和测试集\n",
    "X_val, X_test, y_binary_val, y_binary_test, y_multi_val, y_multi_test = train_test_split(\n",
    "    X_temp, y_binary_temp, y_multi_temp, test_size=0.5, stratify=y_multi_temp, random_state=42\n",
    ")\n",
    "\n",
    "split_time = time.time() - split_time\n",
    "print(f\"数据集划分完成，耗时: {split_time:.2f}秒\")\n",
    "\n",
    "# 显示不同集合的大小和分布\n",
    "print(f\"\\n训练集: {X_train.shape[0]} 样本\")\n",
    "print(f\"验证集: {X_val.shape[0]} 样本\")\n",
    "print(f\"测试集: {X_test.shape[0]} 样本\")\n",
    "\n",
    "print(\"\\n二分类标签分布:\")\n",
    "print(f\"训练集: {Counter(y_binary_train)}\")\n",
    "print(f\"验证集: {Counter(y_binary_val)}\")\n",
    "print(f\"测试集: {Counter(y_binary_test)}\")\n",
    "\n",
    "print(\"\\n多分类标签分布:\")\n",
    "print(f\"训练集: {Counter(y_multi_train)}\")\n",
    "print(f\"验证集: {Counter(y_multi_val)}\")\n",
    "print(f\"测试集: {Counter(y_multi_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. 特征缩放\n",
      "--------------------------------------------------\n",
      "使用RobustScaler进行特征缩放...\n",
      "特征缩放完成，耗时: 9.19秒\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/root/autodl-tmp/projects/DL/dataset/feature_engineering/robust_scaler.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 特征缩放\n",
    "# --------------------------------------------------\n",
    "print(\"\\n6. 特征缩放\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# 使用RobustScaler进行特征缩放以处理异常值\n",
    "print(\"使用RobustScaler进行特征缩放...\")\n",
    "scaling_time = time.time()\n",
    "\n",
    "# 创建并拟合缩放器\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 将缩放后的数据转换回DataFrame以保留列名和索引\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "scaling_time = time.time() - scaling_time\n",
    "print(f\"特征缩放完成，耗时: {scaling_time:.2f}秒\")\n",
    "\n",
    "# 保存缩放器以便后续使用\n",
    "joblib.dump(scaler, os.path.join(OUTPUT_DIR, 'robust_scaler.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. 类别不平衡处理\n",
      "--------------------------------------------------\n",
      "\n",
      "7.1 处理二分类任务的类别不平衡...\n",
      "应用SMOTEENN平衡二分类数据...\n",
      "二分类平衡处理完成，耗时: 9426.92秒\n",
      "平衡前: Counter({0: 1591168, 1: 390352})\n",
      "平衡后: Counter({1: 1579962, 0: 1576401})\n",
      "\n",
      "7.2 处理多分类任务的类别不平衡...\n",
      "多分类任务的原始类别分布:\n",
      "Counter({0: 1591168, 1: 266481, 7: 111251, 3: 5557, 4: 4128, 6: 1376, 5: 1055, 2: 504})\n",
      "对类别 5 进行过采样: 1055 -> 5000\n",
      "对类别 2 进行过采样: 504 -> 5000\n",
      "对类别 6 进行过采样: 1376 -> 5000\n",
      "对类别 4 进行过采样: 4128 -> 5000\n",
      "多分类平衡处理完成，耗时: 7.92秒\n",
      "平衡后的类别分布: Counter({0: 1591168, 1: 266481, 7: 111251, 3: 5557, 5: 5000, 2: 5000, 6: 5000, 4: 5000})\n"
     ]
    }
   ],
   "source": [
    "# 7. 类别不平衡处理\n",
    "# --------------------------------------------------\n",
    "print(\"\\n7. 类别不平衡处理\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# 仅对训练集进行类别平衡处理\n",
    "\n",
    "# 7.1 二分类任务的平衡处理\n",
    "print(\"\\n7.1 处理二分类任务的类别不平衡...\")\n",
    "binary_balance_time = time.time()\n",
    "\n",
    "# 使用SMOTEENN组合过采样和清洗\n",
    "print(\"应用SMOTEENN平衡二分类数据...\")\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_train_binary_balanced, y_binary_train_balanced = smote_enn.fit_resample(X_train_scaled, y_binary_train)\n",
    "\n",
    "binary_balance_time = time.time() - binary_balance_time\n",
    "print(f\"二分类平衡处理完成，耗时: {binary_balance_time:.2f}秒\")\n",
    "print(f\"平衡前: {Counter(y_binary_train)}\")\n",
    "print(f\"平衡后: {Counter(y_binary_train_balanced)}\")\n",
    "\n",
    "# 7.2 多分类任务的平衡处理\n",
    "print(\"\\n7.2 处理多分类任务的类别不平衡...\")\n",
    "multi_balance_time = time.time()\n",
    "\n",
    "# 对于多分类，使用分层策略 - 保持大类样本数量不变，对小类进行上采样\n",
    "# 确定最少需要多少样本\n",
    "min_samples_per_class = 5000  # 每个类别的最小样本数\n",
    "\n",
    "# 获取各类别当前的样本数\n",
    "class_counts = Counter(y_multi_train)\n",
    "print(\"多分类任务的原始类别分布:\")\n",
    "print(class_counts)\n",
    "\n",
    "# 进行分层采样\n",
    "X_multi_resampled = pd.DataFrame()\n",
    "y_multi_resampled = pd.Series()\n",
    "\n",
    "for class_label, count in class_counts.items():\n",
    "    # 获取当前类别的样本\n",
    "    class_indices = y_multi_train[y_multi_train == class_label].index\n",
    "    X_class = X_train_scaled.loc[class_indices]\n",
    "    y_class = y_multi_train.loc[class_indices]\n",
    "    \n",
    "    # 如果样本量少于阈值，进行过采样\n",
    "    if count < min_samples_per_class:\n",
    "        # 计算需要合成的样本数量\n",
    "        n_samples = min_samples_per_class\n",
    "        print(f\"对类别 {class_label} 进行过采样: {count} -> {n_samples}\")\n",
    "        \n",
    "        # 使用带替换的随机采样进行过采样\n",
    "        X_resampled, y_resampled = resample(\n",
    "            X_class, y_class, \n",
    "            replace=True,\n",
    "            n_samples=n_samples,\n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        # 对于大类别，保持原样\n",
    "        X_resampled, y_resampled = X_class, y_class\n",
    "    \n",
    "    # 合并到结果中\n",
    "    X_multi_resampled = pd.concat([X_multi_resampled, X_resampled])\n",
    "    y_multi_resampled = pd.concat([y_multi_resampled, y_resampled])\n",
    "\n",
    "multi_balance_time = time.time() - multi_balance_time\n",
    "print(f\"多分类平衡处理完成，耗时: {multi_balance_time:.2f}秒\")\n",
    "print(f\"平衡后的类别分布: {Counter(y_multi_resampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. 保存处理后的数据集\n",
      "--------------------------------------------------\n",
      "保存二分类数据集...\n",
      "二分类数据集保存完成，耗时: 1.78秒\n",
      "保存多分类数据集...\n",
      "多分类数据集保存完成，耗时: 1.25秒\n",
      "特征列表已保存，共 70 个特征\n"
     ]
    }
   ],
   "source": [
    "# 8. 保存处理后的数据集\n",
    "# --------------------------------------------------\n",
    "print(\"\\n8. 保存处理后的数据集\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# 8.1 保存二分类数据集\n",
    "print(\"保存二分类数据集...\")\n",
    "binary_save_time = time.time()\n",
    "\n",
    "# 训练集（平衡后）\n",
    "binary_train_data = {\n",
    "    'X_train': X_train_binary_balanced,\n",
    "    'y_train': y_binary_train_balanced\n",
    "}\n",
    "joblib.dump(binary_train_data, os.path.join(OUTPUT_DIR, 'binary_train_balanced.joblib'))\n",
    "\n",
    "# 验证集和测试集（保持原始分布）\n",
    "binary_val_data = {\n",
    "    'X_val': X_val_scaled,\n",
    "    'y_val': y_binary_val\n",
    "}\n",
    "joblib.dump(binary_val_data, os.path.join(OUTPUT_DIR, 'binary_val.joblib'))\n",
    "\n",
    "binary_test_data = {\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_test': y_binary_test\n",
    "}\n",
    "joblib.dump(binary_test_data, os.path.join(OUTPUT_DIR, 'binary_test.joblib'))\n",
    "\n",
    "binary_save_time = time.time() - binary_save_time\n",
    "print(f\"二分类数据集保存完成，耗时: {binary_save_time:.2f}秒\")\n",
    "\n",
    "# 8.2 保存多分类数据集\n",
    "print(\"保存多分类数据集...\")\n",
    "multi_save_time = time.time()\n",
    "\n",
    "# 训练集（平衡后）\n",
    "multi_train_data = {\n",
    "    'X_train': X_multi_resampled,\n",
    "    'y_train': y_multi_resampled\n",
    "}\n",
    "joblib.dump(multi_train_data, os.path.join(OUTPUT_DIR, 'multi_train_balanced.joblib'))\n",
    "\n",
    "# 验证集和测试集（保持原始分布）\n",
    "multi_val_data = {\n",
    "    'X_val': X_val_scaled, \n",
    "    'y_val': y_multi_val\n",
    "}\n",
    "joblib.dump(multi_val_data, os.path.join(OUTPUT_DIR, 'multi_val.joblib'))\n",
    "\n",
    "multi_test_data = {\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_test': y_multi_test\n",
    "}\n",
    "joblib.dump(multi_test_data, os.path.join(OUTPUT_DIR, 'multi_test.joblib'))\n",
    "\n",
    "multi_save_time = time.time() - multi_save_time\n",
    "print(f\"多分类数据集保存完成，耗时: {multi_save_time:.2f}秒\")\n",
    "\n",
    "# 8.3 保存特征列表\n",
    "feature_list = X_train.columns.tolist()\n",
    "joblib.dump(feature_list, os.path.join(OUTPUT_DIR, 'feature_list.joblib'))\n",
    "print(f\"特征列表已保存，共 {len(feature_list)} 个特征\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. 数据处理总结\n",
      "--------------------------------------------------\n",
      "总处理时间: 9510.28秒\n",
      "\n",
      "数据集统计:\n",
      "原始数据集: (2830743, 84)\n",
      "特征数量: 70\n",
      "二分类类别: 2\n",
      "多分类类别: 8\n",
      "\n",
      "已完成以下处理:\n",
      "- 数据加载与探索\n",
      "- 创建二分类和多分类标签\n",
      "- 特征工程与选择\n",
      "- 分层数据集划分\n",
      "- 特征缩放\n",
      "- 类别不平衡处理\n",
      "- 保存处理后的数据集\n",
      "\n",
      "处理后的文件已保存至:\n",
      "/root/autodl-tmp/projects/DL/dataset/feature_engineering\n",
      "\n",
      "特征工程和数据准备完成！现在可以继续进行深度学习模型构建。\n"
     ]
    }
   ],
   "source": [
    "# 9. 总结\n",
    "# --------------------------------------------------\n",
    "print(\"\\n9. 数据处理总结\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"总处理时间: {total_time:.2f}秒\")\n",
    "\n",
    "print(\"\\n数据集统计:\")\n",
    "print(f\"原始数据集: {df.shape}\")\n",
    "print(f\"特征数量: {X.shape[1]}\")\n",
    "print(f\"二分类类别: {len(np.unique(y_binary))}\")\n",
    "print(f\"多分类类别: {len(np.unique(y_multi))}\")\n",
    "\n",
    "print(\"\\n已完成以下处理:\")\n",
    "print(\"- 数据加载与探索\")\n",
    "print(\"- 创建二分类和多分类标签\")\n",
    "print(\"- 特征工程与选择\")\n",
    "print(\"- 分层数据集划分\")\n",
    "print(\"- 特征缩放\")\n",
    "print(\"- 类别不平衡处理\")\n",
    "print(\"- 保存处理后的数据集\")\n",
    "\n",
    "print(\"\\n处理后的文件已保存至:\")\n",
    "print(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl-nsl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
