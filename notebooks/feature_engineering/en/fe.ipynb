{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries and Setup Paths\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from collections import Counter\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.utils import resample\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed to ensure reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set better visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Define paths\n",
    "INPUT_PATH = '/root/autodl-tmp/projects/DL/dataset/preprocessed/CICIDS2017_merged_preprocessed.csv'\n",
    "OUTPUT_DIR = '/root/autodl-tmp/projects/DL/dataset/feature_engineering'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"CICIDS2017 Feature Engineering and Stratified Data Preparation\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading and Initial Exploration\n",
    "print(\"\\n1. Loading Preprocessed Data\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Loading data from: {INPUT_PATH}\")\n",
    "\n",
    "# First read a small sample to determine data types\n",
    "sample_df = pd.read_csv(INPUT_PATH, nrows=10000)\n",
    "dtypes = sample_df.dtypes\n",
    "numeric_columns = sample_df.select_dtypes(include=['float64']).columns\n",
    "int_columns = sample_df.select_dtypes(include=['int64']).columns\n",
    "\n",
    "# Set optimized data types\n",
    "optimized_dtypes = {}\n",
    "for col in numeric_columns:\n",
    "    optimized_dtypes[col] = 'float32'  # Reduce precision to save memory\n",
    "for col in int_columns:\n",
    "    optimized_dtypes[col] = 'int32'  # Reduce precision to save memory\n",
    "\n",
    "# Read in batches and merge\n",
    "chunk_size = 500000  # Number of rows to read per batch\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(INPUT_PATH, chunksize=chunk_size, dtype=optimized_dtypes):\n",
    "    chunks.append(chunk)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"Data loading completed, time elapsed: {load_time:.2f} seconds\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Display memory usage\n",
    "memory_usage = df.memory_usage().sum() / (1024 ** 2)\n",
    "print(f\"Memory usage: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Exploratory Data Analysis\n",
    "print(\"\\n2. Exploratory Data Analysis\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "label_counts = df['Label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# Visualize label distribution\n",
    "plt.figure(figsize=(14, 8))\n",
    "label_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Attack Types Distribution')\n",
    "plt.xlabel('Attack Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'attack_distribution.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Get percentage of each attack type\n",
    "attack_percentages = (label_counts / len(df)) * 100\n",
    "print(\"\\nPercentage of each attack type:\")\n",
    "for attack_type, percentage in attack_percentages.items():\n",
    "    print(f\"{attack_type}: {percentage:.4f}%\")\n",
    "\n",
    "# Check data types and missing values\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nChecking for missing values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "if len(missing_values) > 0:\n",
    "    print(missing_values)\n",
    "else:\n",
    "    print(\"No missing values\")\n",
    "\n",
    "# Statistical analysis\n",
    "print(\"\\nStatistical summary of numerical features:\")\n",
    "numeric_df = df.select_dtypes(include=['float32', 'float64', 'int32', 'int64'])\n",
    "summary_stats = numeric_df.describe().T\n",
    "summary_stats['range'] = summary_stats['max'] - summary_stats['min']\n",
    "summary_stats['coefficient_of_variation'] = summary_stats['std'] / summary_stats['mean']\n",
    "print(summary_stats[['mean', 'std', 'min', 'max', 'range', 'coefficient_of_variation']].head())\n",
    "\n",
    "# Save complete statistical summary\n",
    "summary_stats.to_csv(os.path.join(OUTPUT_DIR, 'feature_statistics.csv'))\n",
    "print(f\"Complete statistical summary saved to: {os.path.join(OUTPUT_DIR, 'feature_statistics.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Creating Binary and Multiclass Labels\n",
    "print(\"\\n3. Creating Binary and Multiclass Labels\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Create binary labels (Normal vs Attack)\n",
    "df['binary_label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
    "print(\"Binary labels created, distribution as follows:\")\n",
    "print(df['binary_label'].value_counts())\n",
    "\n",
    "# Create multiclass labels - Group rare attack types\n",
    "# Define grouping strategy - Classify attacks with less than 1000 samples as \"Other Attacks\"\n",
    "THRESHOLD = 1000\n",
    "attack_counts = label_counts[label_counts.index != 'BENIGN']\n",
    "rare_attacks = attack_counts[attack_counts < THRESHOLD].index.tolist()\n",
    "\n",
    "print(f\"\\nThe following rare attack types will be grouped as 'Other Attacks' (threshold: {THRESHOLD}):\")\n",
    "for attack in rare_attacks:\n",
    "    print(f\"- {attack}: {label_counts[attack]}\")\n",
    "\n",
    "# Create multiclass labels\n",
    "def create_multiclass_label(label):\n",
    "    if label == 'BENIGN':\n",
    "        return 'Normal'\n",
    "    elif label in rare_attacks:\n",
    "        return 'Other Attacks'\n",
    "    else:\n",
    "        # Group DOS attack types together\n",
    "        if 'DOS' in label:\n",
    "            return 'DOS'\n",
    "        # Group Web attack types together\n",
    "        elif 'WEB ATTACK' in label:\n",
    "            return 'Web Attack'\n",
    "        else:\n",
    "            return label\n",
    "\n",
    "df['multiclass_label'] = df['Label'].apply(create_multiclass_label)\n",
    "print(\"\\nMulticlass labels created, distribution as follows:\")\n",
    "print(df['multiclass_label'].value_counts())\n",
    "\n",
    "# Encode multiclass labels as numbers\n",
    "label_mapping = {label: idx for idx, label in enumerate(df['multiclass_label'].unique())}\n",
    "df['multiclass_encoded'] = df['multiclass_label'].map(label_mapping)\n",
    "\n",
    "print(\"\\nLabel mapping:\")\n",
    "for label, code in label_mapping.items():\n",
    "    print(f\"{label}: {code}\")\n",
    "\n",
    "# Save label mapping for future use\n",
    "joblib.dump(label_mapping, os.path.join(OUTPUT_DIR, 'label_mapping.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Feature Engineering and Selection\n",
    "print(\"\\n4. Feature Engineering and Selection\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Remove label columns and potential data leakage columns\n",
    "features_to_drop = ['Label', 'binary_label', 'multiclass_label', 'multiclass_encoded', 'Day', 'Scenario']\n",
    "X = df.drop(columns=features_to_drop, errors='ignore')\n",
    "y_binary = df['binary_label']\n",
    "y_multi = df['multiclass_encoded']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Check feature correlation\n",
    "print(\"\\nCalculating feature correlation matrix...\")\n",
    "correlation_time = time.time()\n",
    "\n",
    "# To save memory, only calculate correlation for a subset of features\n",
    "# a. Select numerical columns\n",
    "numeric_cols = X.select_dtypes(include=['float32', 'float64', 'int32', 'int64']).columns\n",
    "\n",
    "# b. If too many features, select top N\n",
    "MAX_CORRELATION_FEATURES = 30  # Limit the number of features for correlation analysis\n",
    "if len(numeric_cols) > MAX_CORRELATION_FEATURES:\n",
    "    # Select features with highest variance\n",
    "    variances = X[numeric_cols].var().sort_values(ascending=False)\n",
    "    selected_cols = variances.index[:MAX_CORRELATION_FEATURES].tolist()\n",
    "    print(f\"Selected {MAX_CORRELATION_FEATURES} high-variance features for correlation analysis\")\n",
    "else:\n",
    "    selected_cols = numeric_cols.tolist()\n",
    "\n",
    "# c. Calculate correlation\n",
    "corr_matrix = X[selected_cols].corr()\n",
    "correlation_time = time.time() - correlation_time\n",
    "print(f\"Correlation calculation completed, time elapsed: {correlation_time:.2f} seconds\")\n",
    "\n",
    "# Save correlation matrix heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'correlation_heatmap.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "corr_pairs = []\n",
    "for i in range(len(selected_cols)):\n",
    "    for j in range(i+1, len(selected_cols)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:  # High correlation threshold set to 0.9\n",
    "            corr_pairs.append((selected_cols[i], selected_cols[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "print(f\"\\nFound {len(corr_pairs)} highly correlated feature pairs (|correlation| > 0.9):\")\n",
    "for feat1, feat2, corr in corr_pairs[:5]:  # Only show first 5 pairs\n",
    "    print(f\"- {feat1} and {feat2}: {corr:.4f}\")\n",
    "\n",
    "if len(corr_pairs) > 5:\n",
    "    print(f\"...and {len(corr_pairs)-5} other highly correlated feature pairs\")\n",
    "\n",
    "# Detect constant and near-constant features\n",
    "const_features = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "near_const_features = [col for col in X.columns if X[col].nunique() <= 2 and X[col].nunique() > 1]\n",
    "\n",
    "print(f\"\\nFound {len(const_features)} constant features and {len(near_const_features)} near-constant features\")\n",
    "if const_features:\n",
    "    print(\"Constant features:\")\n",
    "    for feat in const_features:\n",
    "        print(f\"- {feat}\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    X = X.drop(columns=const_features)\n",
    "    print(f\"Constant features removed, new feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Identify high cardinality features\n",
    "high_cardinality_cols = []\n",
    "for col in X.columns:\n",
    "    unique_ratio = X[col].nunique() / len(X)\n",
    "    if unique_ratio > 0.8:\n",
    "        high_cardinality_cols.append((col, unique_ratio))\n",
    "\n",
    "print(f\"\\nFound {len(high_cardinality_cols)} high cardinality features (unique value ratio > 80%):\")\n",
    "for col, ratio in high_cardinality_cols[:5]:  # Only show first 5\n",
    "    print(f\"- {col}: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Stratified Dataset Split\n",
    "print(\"\\n5. Stratified Dataset Split\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Perform stratified split: training, validation, and test sets (70-15-15)\n",
    "print(\"Performing stratified dataset split...\")\n",
    "split_time = time.time()\n",
    "\n",
    "# First split into training and temporary sets\n",
    "X_train, X_temp, y_binary_train, y_binary_temp, y_multi_train, y_multi_temp = train_test_split(\n",
    "    X, y_binary, y_multi, test_size=0.3, stratify=y_multi, random_state=42\n",
    ")\n",
    "\n",
    "# Then split temporary set into validation and test sets\n",
    "X_val, X_test, y_binary_val, y_binary_test, y_multi_val, y_multi_test = train_test_split(\n",
    "    X_temp, y_binary_temp, y_multi_temp, test_size=0.5, stratify=y_multi_temp, random_state=42\n",
    ")\n",
    "\n",
    "split_time = time.time() - split_time\n",
    "print(f\"Dataset split completed, time elapsed: {split_time:.2f} seconds\")\n",
    "\n",
    "# Display sizes and distributions of different sets\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(\"\\nBinary label distribution:\")\n",
    "print(f\"Training set: {Counter(y_binary_train)}\")\n",
    "print(f\"Validation set: {Counter(y_binary_val)}\")\n",
    "print(f\"Test set: {Counter(y_binary_test)}\")\n",
    "\n",
    "print(\"\\nMulticlass label distribution:\")\n",
    "print(f\"Training set: {Counter(y_multi_train)}\")\n",
    "print(f\"Validation set: {Counter(y_multi_val)}\")\n",
    "print(f\"Test set: {Counter(y_multi_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Feature Scaling\n",
    "print(\"\\n6. Feature Scaling\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Use RobustScaler for feature scaling to handle outliers\n",
    "print(\"Using RobustScaler for feature scaling...\")\n",
    "scaling_time = time.time()\n",
    "\n",
    "# Create and fit scaler\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert scaled data back to DataFrame to preserve column names and indices\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "scaling_time = time.time() - scaling_time\n",
    "print(f\"Feature scaling completed, time elapsed: {scaling_time:.2f} seconds\")\n",
    "\n",
    "# Save scaler for future use\n",
    "joblib.dump(scaler, os.path.join(OUTPUT_DIR, 'robust_scaler.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Class Imbalance Handling\n",
    "print(\"\\n7. Class Imbalance Handling\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Only balance the training set\n",
    "\n",
    "# a. Balance handling for binary classification\n",
    "print(\"\\n7.1 Handling class imbalance for binary classification...\")\n",
    "binary_balance_time = time.time()\n",
    "\n",
    "# Use SMOTEENN for combined oversampling and cleaning\n",
    "print(\"Applying SMOTEENN to balance binary data...\")\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_train_binary_balanced, y_binary_train_balanced = smote_enn.fit_resample(X_train_scaled, y_binary_train)\n",
    "\n",
    "binary_balance_time = time.time() - binary_balance_time\n",
    "print(f\"Binary class balancing completed, time elapsed: {binary_balance_time:.2f} seconds\")\n",
    "print(f\"Before balancing: {Counter(y_binary_train)}\")\n",
    "print(f\"After balancing: {Counter(y_binary_train_balanced)}\")\n",
    "\n",
    "# b. Balance handling for multiclass classification\n",
    "print(\"\\n7.2 Handling class imbalance for multiclass classification...\")\n",
    "multi_balance_time = time.time()\n",
    "\n",
    "# For multiclass, use stratified strategy - keep large classes unchanged, oversample small classes\n",
    "# Determine minimum number of samples needed\n",
    "min_samples_per_class = 5000  # Minimum samples per class\n",
    "\n",
    "# Get current sample count for each class\n",
    "class_counts = Counter(y_multi_train)\n",
    "print(\"Original class distribution for multiclass task:\")\n",
    "print(class_counts)\n",
    "\n",
    "# Perform stratified sampling\n",
    "X_multi_resampled = pd.DataFrame()\n",
    "y_multi_resampled = pd.Series()\n",
    "\n",
    "for class_label, count in class_counts.items():\n",
    "    # Get samples for current class\n",
    "    class_indices = y_multi_train[y_multi_train == class_label].index\n",
    "    X_class = X_train_scaled.loc[class_indices]\n",
    "    y_class = y_multi_train.loc[class_indices]\n",
    "    \n",
    "    # If sample count is less than threshold, oversample\n",
    "    if count < min_samples_per_class:\n",
    "        # Calculate number of samples to synthesize\n",
    "        n_samples = min_samples_per_class\n",
    "        print(f\"Oversampling class {class_label}: {count} -> {n_samples}\")\n",
    "        \n",
    "        # Use random sampling with replacement for oversampling\n",
    "        X_resampled, y_resampled = resample(\n",
    "            X_class, y_class, \n",
    "            replace=True,\n",
    "            n_samples=n_samples,\n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        # For large classes, keep unchanged\n",
    "        X_resampled, y_resampled = X_class, y_class\n",
    "    \n",
    "    # Merge to results\n",
    "    X_multi_resampled = pd.concat([X_multi_resampled, X_resampled])\n",
    "    y_multi_resampled = pd.concat([y_multi_resampled, y_resampled])\n",
    "\n",
    "multi_balance_time = time.time() - multi_balance_time\n",
    "print(f\"Multiclass balancing completed, time elapsed: {multi_balance_time:.2f} seconds\")\n",
    "print(f\"Class distribution after balancing: {Counter(y_multi_resampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Processed Datasets\n",
    "print(\"\\n8. Save Processed Datasets\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# 8.1 Save binary classification datasets\n",
    "print(\"Saving binary classification datasets...\")\n",
    "binary_save_time = time.time()\n",
    "\n",
    "# Training set (balanced)\n",
    "binary_train_data = {\n",
    "    'X_train': X_train_binary_balanced,\n",
    "    'y_train': y_binary_train_balanced\n",
    "}\n",
    "joblib.dump(binary_train_data, os.path.join(OUTPUT_DIR, 'binary_train_balanced.joblib'))\n",
    "\n",
    "# Validation and test sets (keep original distribution)\n",
    "binary_val_data = {\n",
    "    'X_val': X_val_scaled,\n",
    "    'y_val': y_binary_val\n",
    "}\n",
    "joblib.dump(binary_val_data, os.path.join(OUTPUT_DIR, 'binary_val.joblib'))\n",
    "\n",
    "binary_test_data = {\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_test': y_binary_test\n",
    "}\n",
    "joblib.dump(binary_test_data, os.path.join(OUTPUT_DIR, 'binary_test.joblib'))\n",
    "\n",
    "binary_save_time = time.time() - binary_save_time\n",
    "print(f\"Binary classification datasets saved, time elapsed: {binary_save_time:.2f} seconds\")\n",
    "\n",
    "# 8.2 Save multiclass classification datasets\n",
    "print(\"Saving multiclass classification datasets...\")\n",
    "multi_save_time = time.time()\n",
    "\n",
    "# Training set (balanced)\n",
    "multi_train_data = {\n",
    "    'X_train': X_multi_resampled,\n",
    "    'y_train': y_multi_resampled\n",
    "}\n",
    "joblib.dump(multi_train_data, os.path.join(OUTPUT_DIR, 'multi_train_balanced.joblib'))\n",
    "\n",
    "# Validation and test sets (keep original distribution)\n",
    "multi_val_data = {\n",
    "    'X_val': X_val_scaled, \n",
    "    'y_val': y_multi_val\n",
    "}\n",
    "joblib.dump(multi_val_data, os.path.join(OUTPUT_DIR, 'multi_val.joblib'))\n",
    "\n",
    "multi_test_data = {\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_test': y_multi_test\n",
    "}\n",
    "joblib.dump(multi_test_data, os.path.join(OUTPUT_DIR, 'multi_test.joblib'))\n",
    "\n",
    "multi_save_time = time.time() - multi_save_time\n",
    "print(f\"Multiclass classification datasets saved, time elapsed: {multi_save_time:.2f} seconds\")\n",
    "\n",
    "# 8.3 Save feature list\n",
    "feature_list = X_train.columns.tolist()\n",
    "joblib.dump(feature_list, os.path.join(OUTPUT_DIR, 'feature_list.joblib'))\n",
    "print(f\"Feature list saved, total of {len(feature_list)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Summary\n",
    "print(\"\\n9. Data Processing Summary\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Original dataset: {df.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Binary classes: {len(np.unique(y_binary))}\")\n",
    "print(f\"Multiclass classes: {len(np.unique(y_multi))}\")\n",
    "\n",
    "print(\"\\nCompleted processes:\")\n",
    "print(\"- Data loading and exploration\")\n",
    "print(\"- Creation of binary and multiclass labels\")\n",
    "print(\"- Feature engineering and selection\")\n",
    "print(\"- Stratified dataset split\")\n",
    "print(\"- Feature scaling\")\n",
    "print(\"- Class imbalance handling\")\n",
    "print(\"- Saving processed datasets\")\n",
    "\n",
    "print(\"\\nProcessed files saved to:\")\n",
    "print(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl-nsl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
